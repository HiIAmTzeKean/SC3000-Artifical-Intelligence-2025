{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "BkzMRRa5EHdz",
   "metadata": {
    "id": "BkzMRRa5EHdz"
   },
   "source": [
    "# SC3000 Lab Assignment 1: Teaching NanoGPT to Do Math\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook applies **Direct Preference Optimization (DPO)** to fine-tune a pretrained NanoGPT for mathematical reasoning. DPO trains the language model to favor better responses over weaker ones—without building a separate reward model.\n",
    "\n",
    "### What is DPO?\n",
    "\n",
    "**Direct Preference Optimization** is a training approach that replaces the RLHF pipeline. Rather than learning a reward model and using reinforcement learning, DPO updates the model directly from paired examples where one response is preferred over another.\n",
    "\n",
    "### Why DPO?\n",
    "\n",
    "* **Simpler pipeline**: Removes the need for a reward model and RL steps\n",
    "* **More stable**: Avoids the instabilities common in RL-based training\n",
    "* **Strong results**: Often matches or beats RLHF performance\n",
    "* **Resource-friendly**: Lower computational overhead\n",
    "\n",
    "### Data Setup\n",
    "\n",
    "Training uses **preference pairs**:\n",
    "\n",
    "* **Positive (preferred)**: Correct solutions with clear reasoning\n",
    "* **Negative (dispreferred)**: Incorrect answers or weak/illogical reasoning\n",
    "\n",
    "The objective pushes the model to assign higher likelihood to positive responses and lower likelihood to negative ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fYV0A6jgGH3V",
   "metadata": {
    "id": "fYV0A6jgGH3V"
   },
   "source": [
    "## The DPO Algorithm: Mathematical Intuition\n",
    "\n",
    "### Core Idea\n",
    "\n",
    "In classic RLHF, you typically:\n",
    "\n",
    "1. Train a reward model on human preferences\n",
    "2. Use that reward model to score candidate outputs\n",
    "3. Run RL (e.g., PPO) to improve the policy\n",
    "\n",
    "**DPO** collapses this into a *single* stage: it updates the policy (language model) *directly* from preference pairs.\n",
    "\n",
    "### The DPO Loss Function (Explained)\n",
    "\n",
    "Given a preference pair (y_pos, y_neg) where y_pos is preferred over y_neg:\n",
    "\n",
    "$$\\mathcal{L}_{DPO} = -\\mathbb{E}\\left[\\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta(y_{pos}|x)}{\\pi_{ref}(y_{pos}|x)} - \\beta \\log \\frac{\\pi_\\theta(y_{neg}|x)}{\\pi_{ref}(y_{neg}|x)}\\right)\\right]$$\n",
    "\n",
    "**Term meanings:**\n",
    "\n",
    "* *(\\pi_\\theta)*: the policy we’re training\n",
    "* *(\\pi_{\\text{ref}})*: a reference policy (often the current/initial model)\n",
    "* *(\\beta)*: temperature scaling how strongly preferences shape updates\n",
    "* *(\\sigma)*: the sigmoid mapping the log-odds difference to a probability\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Pushes the model to assign *higher* likelihood to preferred responses\n",
    "* Penalizes giving *higher* likelihood to dispreferred ones\n",
    "* *(\\beta)* adjusts the *strength* of these updates\n",
    "\n",
    "### Why DPO Works\n",
    "\n",
    "1. *Direct optimization*: skips reward-model + RL loops\n",
    "2. *Stable*: avoids RL-induced instability\n",
    "3. *Principled*: connects back to the RLHF objective\n",
    "4. *Efficient*: one-stage training\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a869a",
   "metadata": {
    "id": "124a869a"
   },
   "source": [
    "## Step 1: Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wNdLQGy7a5XK",
   "metadata": {
    "id": "wNdLQGy7a5XK"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6QbDpreGwTMa",
   "metadata": {
    "id": "6QbDpreGwTMa"
   },
   "outputs": [],
   "source": [
    "%cd \"/content/drive/MyDrive/Colab Notebooks/NanoGPT-Math\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82f8f1",
   "metadata": {
    "id": "3b82f8f1"
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rC-Wa8Euw4hj",
   "metadata": {
    "id": "rC-Wa8Euw4hj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fIY4svJeH-r4",
   "metadata": {
    "id": "fIY4svJeH-r4"
   },
   "source": [
    "## Hyperparameter Choices and Their Impact\n",
    "\n",
    "| Parameter             |           Value | Impact                                                                  |\n",
    "| --------------------- | --------------: | ----------------------------------------------------------------------- |\n",
    "| **beta**              |             0.5 | Preference strength; lower = stronger push, higher = more conservative. |\n",
    "| **learning_rate**     |            1e-4 | Stable fine-tuning; too high risks divergence.                          |\n",
    "| **batch_size**        |              64 | Smoother grads with size; needs more VRAM.                              |\n",
    "| **max_length** |              64 | Caps input; trim long prompts upstream.                                 |\n",
    "| **temperature**       |             0.8 | Randomness: <1 deterministic, >1 diverse.                               |\n",
    "\n",
    "\n",
    "**Key trade-offs:**\n",
    "\n",
    "* **Higher β** → More conservative, closer to reference policy\n",
    "* **Lower β** → More aggressive preference learning (risk of overfitting/drift)\n",
    "* **Larger batch_size** → Smoother gradients, higher VRAM demand\n",
    "* **Higher learning_rate** → Faster convergence, greater instability risk\n",
    "* **Lower temperature / smaller top_k** → Safer, more deterministic generations (possible blandness)\n",
    "* **More epochs / larger max_new_tokens** → Better coverage and completeness, higher compute and overfit risk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2d9de0",
   "metadata": {
    "id": "6c2d9de0"
   },
   "source": [
    "## Step 2: Package imports and configuration\n",
    "\n",
    "### Configuration Parameters\n",
    "\n",
    "* **`beta`**: DPO scaling temperature—tunes how strongly the policy is pushed away from the reference model\n",
    "* **`base_lr`**: Optimizer step size (learning rate)\n",
    "* **`epochs`**: Upper bound on full passes over the training data\n",
    "* **`batch_size`**: Count of preference pairs processed per optimization step\n",
    "* **`max_length`**: Cap on input sequence length\n",
    "* **`temperature`**: Generation randomness control (smaller values → more deterministic outputs)\n",
    "* **`top_k`**: Limits sampling to the top *k* most probable tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876dd92d",
   "metadata": {
    "id": "876dd92d"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from model import GPT, GPTConfig\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# DPO hyperparameters\n",
    "beta = 0.5\n",
    "base_lr = 1e-4\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "# Model parameters\n",
    "max_length =64\n",
    "num_samples = 1\n",
    "max_new_tokens = 200\n",
    "temperature = 0.8\n",
    "top_k = 200\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "with open(\"sft/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "def decode(l): return ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d35e6",
   "metadata": {
    "id": "4c7d35e6"
   },
   "source": [
    "## Step 3: Define Helper Functions\n",
    "\n",
    "We’ll implement three core utilities to support DPO training:\n",
    "\n",
    "### 1. `compute_logprob(input_ids)`\n",
    "\n",
    "Calculates the log-likelihood of a given token sequence under the current model—this feeds directly into the DPO objective.\n",
    "\n",
    "**Intuition**: It quantifies how confident the model is in a sequence. Larger log-probability ⇒ the model finds the sequence more plausible.\n",
    "\n",
    "### 2. `pad_or_truncate(seq, max_length)`\n",
    "\n",
    "Normalizes sequence length by trimming long sequences and padding shorter ones with zeros so tensors align in a batch.\n",
    "\n",
    "### 3. `get_batches(lines, batch_size)`\n",
    "\n",
    "Builds mini-batches of preference pairs for training. Each batch bundles matched **(positive, negative)** examples—where the positive is the correct reasoning/answer and the negative is the incorrect one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d03655c3",
   "metadata": {
    "id": "d03655c3"
   },
   "outputs": [],
   "source": [
    "def compute_logprob(input_ids):\n",
    "    inputs = input_ids[:, :-1]\n",
    "    targets = input_ids[:, 1:]\n",
    "    logits, _ = gpt(inputs, full_seq=True)\n",
    "    B, T, V = logits.size()\n",
    "    logits_flat = logits.reshape(-1, V)\n",
    "    targets_flat = targets.reshape(-1)\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
    "    loss = loss.reshape(B, T)\n",
    "    attention_mask = (targets != 0).float()\n",
    "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return -loss\n",
    "\n",
    "def pad_or_truncate(seq, max_length):\n",
    "    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
    "\n",
    "def get_batches(lines, batch_size):\n",
    "    random.shuffle(lines)\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i+batch_size]\n",
    "        if len(batch) < batch_size:\n",
    "            continue\n",
    "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
    "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
    "        yield neg_tensor, pos_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d9eba",
   "metadata": {
    "id": "fc9d9eba"
   },
   "source": [
    "## Step 4: Load the pretrained NanoGPT model\n",
    "Bring in a NanoGPT checkpoint that’s already been pretrained to use as the initialization for DPO fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceae772a",
   "metadata": {
    "id": "ceae772a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(74, 348)\n",
       "    (wpe): Embedding(256, 348)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n",
       "          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"sft/gpt.pt\", map_location=device)\n",
    "gptconf = GPTConfig(**ckpt['model_args'])\n",
    "gpt = GPT(gptconf)\n",
    "state_dict = ckpt['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k in list(state_dict.keys()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "gpt.to(device).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cQz1CNxcVjQ4",
   "metadata": {
    "id": "cQz1CNxcVjQ4"
   },
   "source": [
    "## Step 5: Load Data\n",
    "\n",
    "Import the DPO dataset of **preference pairs**. Each record includes:\n",
    "\n",
    "* **`positive`**: a correct solution with clear, valid reasoning\n",
    "* **`negative`**: an incorrect answer or weak/flawed reasoning\n",
    "\n",
    "The corpus contains **100,000 pairs**, giving ample supervision for the model to learn preference alignment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91e2c34",
   "metadata": {},
   "source": [
    "### Create the data file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f3b724",
   "metadata": {},
   "source": [
    "Generate problem set using deterministic prompts and store them in a text file. Each line should contain a JSON object with `positive` and `negative` fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1_73NpPQvHxl",
   "metadata": {
    "id": "1_73NpPQvHxl"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def _int_div_triplet():\n",
    "    b = random.randint(1, 12)\n",
    "    x = random.randint(1, 12)\n",
    "    a = b * x\n",
    "    return a, b, x\n",
    "\n",
    "def generate_problem():\n",
    "    mode = random.choice([\n",
    "        \"arith_add\",\"arith_sub\",\"arith_mul\",\"arith_div\",\n",
    "        \"alg_x_mul\",\"alg_mul_x\",\"alg_x_add\",\"alg_add_x\",\n",
    "        \"alg_x_sub\",\"alg_sub_x\",\"alg_div_x\",\"alg_x_div\"\n",
    "    ])\n",
    "\n",
    "    if mode == \"arith_add\":\n",
    "        a,b = random.randint(-50,99), random.randint(-50,99)\n",
    "        ans = a + b; prompt = f\"{a}+{b}=?\"\n",
    "        expl = f\"{a}+{b} equals {ans}.\"\n",
    "    elif mode == \"arith_sub\":\n",
    "        a,b = random.randint(-50,99), random.randint(-50,99)\n",
    "        ans = a - b; prompt = f\"{a}-{b}=?\"\n",
    "        expl = f\"{a}-{b} equals {ans}.\"\n",
    "    elif mode == \"arith_mul\":\n",
    "        a,b = random.randint(-12,12), random.randint(-12,12)\n",
    "        ans = a * b; prompt = f\"{a}*{b}=?\"\n",
    "        expl = f\"{a}*{b} equals {ans}.\"\n",
    "    elif mode == \"arith_div\":\n",
    "        a,b,x = _int_div_triplet()\n",
    "        ans = a // b; prompt = f\"{a}/{b}=?\"\n",
    "        expl = f\"{a}/{b} equals {ans}.\"\n",
    "    elif mode == \"alg_x_mul\":            # x*b=a\n",
    "        a,b,x = _int_div_triplet()\n",
    "        ans = x; prompt = f\"x*{b}={a}, x=?\"\n",
    "        expl = f\"{a}/{b} equals {ans}.\"\n",
    "    elif mode == \"alg_mul_x\":            # b*x=a\n",
    "        a,b,x = _int_div_triplet()\n",
    "        ans = x; prompt = f\"{b}*x={a}, x=?\"\n",
    "        expl = f\"{a}/{b} equals {ans}.\"\n",
    "    elif mode == \"alg_x_add\":            # x+b=a\n",
    "        b = random.randint(-50,99); x = random.randint(-50,99); a = x + b\n",
    "        ans = x; prompt = f\"x+{b}={a}, x=?\"\n",
    "        expl = f\"{a}-{b} equals {ans}.\"\n",
    "    elif mode == \"alg_add_x\":            # b+x=a\n",
    "        b = random.randint(-50,99); x = random.randint(-50,99); a = b + x\n",
    "        ans = x; prompt = f\"{b}+x={a}, x=?\"\n",
    "        expl = f\"{a}-{b} equals {ans}.\"\n",
    "    elif mode == \"alg_x_sub\":            # x-b=a => x=a+b\n",
    "        b = random.randint(-50,99); x = random.randint(-50,99); a = x - b\n",
    "        ans = x; prompt = f\"x-{b}={a}, x=?\"\n",
    "        expl = f\"{a}+{b} equals {ans}.\"\n",
    "    elif mode == \"alg_sub_x\":            # b-x=a => x=b-a\n",
    "        a = random.randint(-50,99); b = random.randint(-50,99); x = b - a\n",
    "        ans = x; prompt = f\"{b}-x={a}, x=?\"\n",
    "        expl = f\"{b}-{a} equals {ans}.\"\n",
    "    elif mode == \"alg_div_x\":            # a/x=b => x=a/b\n",
    "        a,b,x = _int_div_triplet()\n",
    "        ans = x; prompt = f\"{a}/x={b}, x=?\"\n",
    "        expl = f\"{a}/{b} equals {ans}.\"\n",
    "    else:                                # x/b=a => x=a*b\n",
    "        b = random.randint(1,12); a = random.randint(-12,12); x = a*b\n",
    "        ans = x; prompt = f\"x/{b}={a}, x=?\"\n",
    "        expl = f\"{a}*{b} equals {ans}.\"\n",
    "\n",
    "    return prompt, ans, expl\n",
    "\n",
    "def build_positive(prompt: str, ans: int, expl: str) -> str:\n",
    "    return f\"{prompt} The answer is {ans} because {expl}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be22ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here we run the function to generate the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "l0zz5y4-vPjA",
   "metadata": {
    "id": "l0zz5y4-vPjA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:01<00:00, 894148.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1000000 pairs → data/pos_neg_pairs.json\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "N_SAMPLES = 1000_000\n",
    "OUT_PATH  = \"data/pos_neg_pairs.json\"\n",
    "os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
    "\n",
    "pairs = []\n",
    "for _ in tqdm(range(N_SAMPLES)):\n",
    "    prompt, ans, expl = generate_problem()\n",
    "\n",
    "    neg_text = \"Sorry, I do not know!\"\n",
    "    negative = f\"{prompt} {neg_text}\"\n",
    "    positive = build_positive(prompt, ans, expl)\n",
    "\n",
    "    pairs.append({\"negative\": negative, \"positive\": positive})\n",
    "\n",
    "with open(OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(pairs, f, ensure_ascii=True, indent=2)\n",
    "\n",
    "print(f\"Generated {len(pairs)} pairs → {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad02ffd",
   "metadata": {},
   "source": [
    "### Load the data file\n",
    "\n",
    "Here we load the data file into memory for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7edf3d44",
   "metadata": {
    "id": "7edf3d44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 1000000 data pairs from data/pos_neg_pairs.json\n",
      "First pair: {'negative': '52+10=? Sorry, I do not know!', 'positive': '52+10=? The answer is 62 because 52+10 equals 62.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "data_path = \"data/pos_neg_pairs.json\"\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = json.load(f)\n",
    "\n",
    "print(f\"Successfully loaded {len(lines)} data pairs from {data_path}\")\n",
    "print(\"First pair:\", lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5f81f",
   "metadata": {
    "id": "c2e5f81f"
   },
   "source": [
    "## Step 6: Configure the Optimizer and LR Schedule\n",
    "\n",
    "### AdamW Optimizer\n",
    "\n",
    "Use **AdamW**, the go-to optimizer for Transformers. It adds **decoupled weight decay**, which acts as regularization to curb overfitting while keeping Adam’s adaptive updates.\n",
    "\n",
    "### Cosine Annealing Schedule\n",
    "\n",
    "Apply a **cosine decay** to the learning rate so it:\n",
    "\n",
    "* Begins with relatively larger steps for quick early gains\n",
    "* Tapers to smaller steps for fine-grained refinement\n",
    "* Improves convergence by reducing the chance of overshooting the optimum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ANnzVfTCPNai",
   "metadata": {
    "id": "ANnzVfTCPNai"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer and scheduler created.\n",
      "Total training steps: 125000\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "base_lr = 1e-4\n",
    "\n",
    "num_batches = len(lines) // batch_size\n",
    "total_steps = num_batches * epochs\n",
    "\n",
    "optimizer = AdamW(gpt.parameters(), lr=base_lr)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.15 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Optimizer and scheduler created.\")\n",
    "print(f\"Total training steps: {total_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hO-b1CRwPZ3-",
   "metadata": {
    "id": "hO-b1CRwPZ3-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW optimizer and CosineAnnealingLR scheduler created successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "optimizer = optim.AdamW(gpt.parameters(), lr=base_lr, weight_decay=0.10, betas=(0.9, 0.95), eps=1e-8)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
    "\n",
    "print(\"AdamW optimizer and CosineAnnealingLR scheduler created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b66199",
   "metadata": {
    "id": "52b66199"
   },
   "source": [
    "## Step 7: Begin training (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fda178",
   "metadata": {},
   "source": [
    "Check if there is a GPU available and use it if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5XQT-yT8Ug5w",
   "metadata": {
    "id": "5XQT-yT8Ug5w"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 21:38:18.918721: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Max\n",
      "2025-10-26 21:38:18.918744: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 36.00 GB\n",
      "2025-10-26 21:38:18.918748: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 13.50 GB\n",
      "2025-10-26 21:38:18.918764: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-10-26 21:38:18.918774: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6da483",
   "metadata": {},
   "source": [
    "Here we make sure that the model is trained with safe fallback characters to avoid issues with unknown tokens during training.\n",
    "\n",
    "We let the safe character be a space if it exists in the vocabulary; otherwise, we use a newline character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UcZEeRqWB0M3",
   "metadata": {
    "id": "UcZEeRqWB0M3"
   },
   "outputs": [],
   "source": [
    "SAFE_CHAR = ' ' if ' ' in stoi else '\\n'\n",
    "\n",
    "\n",
    "def encode(s):\n",
    "    fallback_id = stoi[SAFE_CHAR]\n",
    "    return [stoi.get(c, fallback_id) for c in s]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c678d4af",
   "metadata": {},
   "source": [
    "Load the last checkpoint for fine-tuning. Skip if starting fresh.\n",
    "\n",
    "This allows us to resume training from where we left off, preserving learned weights and optimizer state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d993ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"dpo/dpo.pt\", map_location=device)\n",
    "gptconf = GPTConfig(**ckpt['model_args'])\n",
    "gpt = GPT(gptconf)\n",
    "state_dict = ckpt['model_state_dict']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k in list(state_dict.keys()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "gpt.to(device).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d1136",
   "metadata": {},
   "source": [
    "The training loop iterates over epochs and batches produced by `get_batches(lines, batch_size)`. For each batch—a pair of tensors `(neg_tensor, pos_tensor)` with shape `(B, T)`—the code moves tensors to `device`, zeroes `optimizer` gradients, computes per-example log-probabilities with `compute_logprob()`, forms the DPO loss, backpropagates, clips gradients with `clip_grad_norm_`, and calls `optimizer.step()`. A `tqdm` progress bar displays the current batch loss and learning rate; the scheduler (if present) is stepped once per epoch, and the model is checkpointed to dpo.pt.\n",
    "\n",
    "Conceptually, DPO directly pushes the model to assign higher likelihood to preferred (positive) outputs than to dispreferred (negative) ones. For each pair the scalar preference score is the difference in per-sequence log-probabilities scaled by `beta`. The batch loss minimized is the negative log-probability of preferring the positive response:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{B}\\sum_{i=1}^B \\log \\sigma\\!\\left(\\frac{\\text{pos\\_logprob}_i - \\text{neg\\_logprob}_i}{\\beta}\\right)\n",
    "$$\n",
    "\n",
    "Equivalently, in code the per-batch loss is written as:\n",
    "`loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ebeb4",
   "metadata": {
    "id": "1d4ebeb4"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "\n",
    "# if beta wasn't set earlier, provide a sensible default\n",
    "beta = 0.3 if 'beta' not in globals() else beta\n",
    "\n",
    "total_steps = len(lines) // batch_size\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    pbar = tqdm(get_batches(lines, batch_size), total=total_steps, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for step, (neg_tensor, pos_tensor) in enumerate(pbar, start=1):\n",
    "        # 1) move to device\n",
    "        neg_tensor = neg_tensor.to(device)\n",
    "        pos_tensor = pos_tensor.to(device)\n",
    "\n",
    "        # 2) zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 3) forward: per-sample log-probabilities\n",
    "        neg_logprob = compute_logprob(neg_tensor)\n",
    "        pos_logprob = compute_logprob(pos_tensor)\n",
    "\n",
    "        # 4) DPO-style loss\n",
    "        loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - pos_logprob.mean() * 0.1\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # progress\n",
    "        lr_now = optimizer.param_groups[0]['lr']\n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix(epoch_loss=f\"{epoch_loss:.4f}\", lr=f\"{lr_now:.2e}\")\n",
    "\n",
    "        # keep the progress bar aligned with total_steps\n",
    "        if step >= total_steps:\n",
    "            break\n",
    "\n",
    "    # save checkpoint (guard model_args in case ckpt isn't in scope)\n",
    "    ckpt_path = f\"dpo/dpo.pt\"\n",
    "    model_args = ckpt['model_args'] if ('ckpt' in globals() and isinstance(ckpt, dict) and 'model_args' in ckpt) else None\n",
    "    torch.save({\n",
    "        \"model_state_dict\": gpt.state_dict(),\n",
    "        \"model_args\": model_args,\n",
    "    }, ckpt_path)\n",
    "    print(f\"Saved checkpoint to {ckpt_path} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f2ab",
   "metadata": {
    "id": "48b7f2ab"
   },
   "source": [
    "## Step 8: Model Evaluation and Testing\n",
    "\n",
    "Test the model on a small set of example problems covering different opeartion types. This quick test helps verify the model is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09027262",
   "metadata": {
    "id": "09027262"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17+19=? → The answer is 26 because 17+19 equals 26.\n",
      "3*17=? → The answer is 51 because 3*17 equals 51.\n",
      "72/4=? → The answer is 13 because 72/4 equals 13.\n",
      "72-x=34, x=? → The answer is 38 because 72-34 equals to 38.\n",
      "x*11=44, x=? → The answer is 44 because 44/11 equals to 44.\n",
      "3*17=? → The answer is 51 because 3*17 equals 51.\n",
      "72/4=? → The answer is 13 because 72/4 equals 13.\n",
      "72-x=34, x=? → The answer is 38 because 72-34 equals to 38.\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1) Load the fine-tuned model\n",
    "ckpt_path = \"dpo/dpo.pt\"\n",
    "assert os.path.exists(ckpt_path), f\"Checkpoint not found at {ckpt_path}. Run Step 7 first.\"\n",
    "\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "gpt = GPT(gptconf).to(device)\n",
    "\n",
    "# state dict (supports either key name)\n",
    "state_dict = checkpoint.get('model', checkpoint.get('model_state_dict'))\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k in list(state_dict.keys()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "gpt.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "# 2) Test\n",
    "gpt.eval()\n",
    "test_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34, x=?\", \"x*11=44, x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34, x=?\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for prompt in test_set:\n",
    "        prompt_ids = encode(prompt)\n",
    "        x_ids = prompt_ids[-max_length:]\n",
    "        x = torch.tensor([x_ids], dtype=torch.long, device=device)\n",
    "\n",
    "        # generate continuation\n",
    "        y, _ = gpt.generate(\n",
    "            x,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "        # decode: print only the newly generated part (cleaner), plus an optional full line\n",
    "        out_ids = y[0].tolist()\n",
    "        new_tokens_only = out_ids[len(x_ids):]\n",
    "        out = decode(new_tokens_only)\n",
    "\n",
    "        print(f\"{prompt} → {out.strip()}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
